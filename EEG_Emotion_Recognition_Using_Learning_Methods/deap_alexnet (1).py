# -*- coding: utf-8 -*-
"""DEAP - Alexnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hMiTM3k_f7bxE7CdWsLYEHkXdwLVzZrO
"""

!pip install mne

# â”€â”€ Standart KÃ¼tÃ¼phaneler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import argparse
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import shutil
from typing import Any, Dict, List, Tuple

# â”€â”€ Bilimsel / SayÄ±sal Ekosistem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import pandas as pd
from scipy.signal import welch
from scipy.stats import skew, kurtosis

import matplotlib
matplotlib.use("Agg")          # GUI'siz arka uÃ§
import matplotlib.pyplot as plt
import seaborn as sns
import mne

# â”€â”€ Makine / Derin Ã–ÄŸrenme â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.preprocessing.image import (
    ImageDataGenerator, load_img, img_to_array
)

from sklearn.model_selection import (
    train_test_split, KFold, cross_validate
)
from sklearn.metrics import (
    mean_absolute_error, r2_score,
    accuracy_score, precision_score,
    recall_score, f1_score, confusion_matrix
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR

import xgboost as xgb
from xgboost import XGBRegressor

# â”€â”€ Excel Ã‡Ä±kÄ±ÅŸÄ± Ä°Ã§in â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import openpyxl

matplotlib.use("Agg")          # GUI'siz, headless backend

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", required=True)
    ap.add_argument("--out", default="outputs_batch")
    ap.add_argument("--subject", type=int, help="YalnÄ±zca belirtilen deneÄŸi iÅŸler (1-32)")
    ap.add_argument("--cv", action="store_true", help="%%20/%%80 hold-out yerine 5-Fold CV ile deÄŸerlendir.")
    args = ap.parse_args()

    data_dir = Path(args.data_dir).expanduser().resolve()
    out_root = Path(args.out).expanduser().resolve()
    out_root.mkdir(parents=True, exist_ok=True)

    cnn_val_dir = out_root / C.CNN_VAL_DIR
    cnn_aro_dir = out_root / C.CNN_ARO_DIR

    # Hangi denekler iÅŸlenecek?
    if args.subject:  # tek denek modu
        idxs = [args.subject]
    else:  # tam kohort
        idxs = range(1, 33)

    tasks = [
        (i,
         data_dir / f"s{i:02d}.dat",
         out_root / f"subj_{i:02d}",
         cnn_val_dir,
         cnn_aro_dir,
         args.cv)
        for i in idxs
    ]

    all_metrics, cm_tot = [], {m: {'val': np.zeros((2,2), int),
                                   'aro': np.zeros((2,2), int)}
                               for m in C.MODELS}

    with ProcessPoolExecutor() as ex:
        futs = {ex.submit(process_subject, *t): t[0] for t in tasks if t[1].exists()}
        for fut in as_completed(futs):
            idx = futs[fut]
            print(f"âœ“ subj_{idx:02d}")
            mets, cms, _ = fut.result()
            all_metrics.append(mets)
            for m in C.MODELS:
                cm_tot[m]['val'] += cms[m][0]
                cm_tot[m]['aro'] += cms[m][1]

    # ----- toplu sonuÃ§lar -----
    df = pd.DataFrame(all_metrics)
    df.loc['mean'] = df.mean()
    df.to_csv(out_root / "metrics.csv", index=False)
    print(df.loc['mean'].round(3))

if __name__ == "__main__":
    import tensorflow as tf
    tf.get_logger().setLevel("ERROR")
    main()

# DSP & sinyal
FS        = 128
WIN_SEC   = 4
N_CHANNELS = 12
THRESH     = 4.5

BANDS = {
    "delta": (1, 4),
    "theta": (4, 8),
    "alpha": (8, 12),
    "beta":  (12, 30),
    "gamma": (30, 45),
}

DEAP_CH_NAMES = [
    "F7", "Fp1", "Fp2", "F8",
    "F3",  "F4",
    "P3",  "P4",
    "P7", "O1", "O2", "P8",
]

MODELS      = ['rf','xgb','svr','knn','dnn','alexnet']
MODELS_IMP  = ['rf','xgb','svr','knn']
STATS       = ["activity","mobility","complexity","skew","kurt"] + list(BANDS.keys())

# Ã‡Ä±ktÄ± dizinlerinin isimleri
CNN_VAL_DIR = "cnn_valence"
CNN_ARO_DIR = "cnn_arousal"

ROOT = Path(__file__).resolve().parent

def load_deap_dat(path: Path) -> Tuple[np.ndarray, np.ndarray]:
    with path.open("rb") as f:
        d = pickle.load(f, encoding="latin1")
    data   = d["data"].astype(np.float32)[:, :N_CHANNELS, :]
    labels = d["labels"][:, :2].astype(np.float32)
    if data.shape[-1] == 8064:
        data = data[:, :, 3*FS:]          # ilk 3 s-i at
    assert data.shape == (40, N_CHANNELS, 60*FS)
    return data, labels

def segment_trials(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    samp_win = WIN_SEC * FS
    X, y = [], []
    for t in range(data.shape[0]):
        for w in range(60 // WIN_SEC):
            s, e = w * samp_win, (w + 1) * samp_win
            X.append(data[t, :, s:e])
            y.append(labels[t])
    return np.stack(X), np.stack(y)

def _bandpower(epoch: np.ndarray, band: Tuple[int, int]) -> float:
    freqs, psd = welch(epoch, fs=FS, nperseg=FS*2)
    fmin, fmax = band
    return psd[(freqs >= fmin) & (freqs <= fmax)].mean()

def extract_features(X_seg: np.ndarray) -> pd.DataFrame:
    n_win, n_ch, _ = X_seg.shape
    feats = []
    for w in range(n_win):
        ch_feats = []
        for ch in range(n_ch):
            sig = X_seg[w, ch]
            var_sig = np.var(sig)
            diff1   = np.diff(sig); var_d1 = np.var(diff1)
            mobility = np.sqrt(var_d1 / var_sig) if var_sig else 0
            diff2    = np.diff(diff1); var_d2 = np.var(diff2)
            mobility2 = np.sqrt(var_d2 / var_d1) if var_d1 else 0
            complexity = mobility2 / mobility if mobility else 0
            ch_feats += [
                var_sig, mobility, complexity,
                skew(sig), kurtosis(sig)
            ]
            ch_feats += [_bandpower(sig, b) for b in BANDS.values()]
        feats.append(ch_feats)

    cols = [f"{stat}_{ch}" for ch in range(n_ch) for stat in STATS]
    return pd.DataFrame(feats, columns=cols)

def regression(y_true, y_pred):
    return {
        'mae_val': mean_absolute_error(y_true[:,0], y_pred[:,0]),
        'mae_aro': mean_absolute_error(y_true[:,1], y_pred[:,1]),
        'r2_val' : r2_score(y_true[:,0],  y_pred[:,0]),
        'r2_aro' : r2_score(y_true[:,1],  y_pred[:,1]),
    }

def conf_mats(y_true, y_pred_bin):
    cm_v = confusion_matrix(y_true[:,0], y_pred_bin[:,0])
    cm_a = confusion_matrix(y_true[:,1], y_pred_bin[:,1])
    return cm_v, cm_a

def binarize(arr, thresh):
    return (arr > thresh).astype(int)

def evaluate_cv(
    pipeline,
    X: np.ndarray,
    y: np.ndarray,
    cv_splits: int = 5
) -> tuple[float, float]:
    """
    Runs KFold CV on (X, y) with the given pipeline.
    Returns (mean_test_mae, mean_test_r2).
    """
    cv = KFold(n_splits=cv_splits, shuffle=True, random_state=42)
    scoring = {
        'mae': make_scorer(mean_absolute_error),
        'r2' : make_scorer(r2_score),
    }
    cv_res = cross_validate(
        pipeline, X, y,
        cv=cv,
        scoring=scoring,
        return_train_score=False,
        n_jobs=-1
    )
    return cv_res['test_mae'].mean(), cv_res['test_r2'].mean()

def _train_all(
    feats: np.ndarray,
    y: np.ndarray,
    cv: bool = False
) -> Dict[str, Tuple[object, Tuple[np.ndarray, np.ndarray, np.ndarray]]]:
    """
    RF, XGB, SVR, k-NN ve DNNâ€™i eÄŸitir; doÄŸrulama kÃ¼mesindeki
    tahminleri dÃ¶ner.  AlexNet buraya **eklenmiyor** â€“ aÅŸaÄŸÄ±da,
    process_subject iÃ§inde Ã§aÄŸrÄ±lacak.
    """
    X_tr, X_val, y_tr, y_val = train_test_split(
        feats, y, test_size=0.2, random_state=42
    )

    results: Dict[str, Tuple[object, Tuple[np.ndarray, np.ndarray, np.ndarray]]] = {}

    # -- Klasik modeller -------------------------------------------------
    for name in C.MODELS_IMP:              # ['rf','xgb','svr','knn']
        pipe = getattr(M, name)()
        pipe.fit(X_tr, y_tr)
        preds = pipe.predict(X_val)
        results[name] = (pipe, (X_val, y_val, preds))

    # -- DNN --------------------------------------------------------------
    rf_preds = results["rf"][0].predict(feats)    # RF Ã§Ä±ktÄ±larÄ±nÄ± ek Ã¶zellik yap
    dnn_model, dnn_res = D.train(feats, y, rf_preds)
    results["dnn"] = (dnn_model, dnn_res)

    return results



def process_subject(
    idx: int,
    dat_path: Path,
    out_dir: Path,
    cnn_val_dir: Path,
    cnn_aro_dir: Path,
    cv: bool = False,
) -> Tuple[Dict, Dict, Dict]:
    """
    Tek bir denek iÃ§in:
      1) Veri yÃ¼kle & segmentle
      2) Ã–zellik Ã§Ä±kar, kaydet
      3) Topomapâ€™larÄ± oluÅŸtur ve cnn klasÃ¶rlerine kopyala
      4) Modelleri eÄŸit & tahmin et
      5) Regresyon & sÄ±nÄ±flandÄ±rma metriklerini hesapla
      6) TÃ¼m grafik Ã§Ä±ktÄ±larÄ± alt klasÃ¶rlere kaydet
    """
    # 0) ALT KLASÃ–RLERÄ° HAZIRLA
    out_dir.mkdir(parents=True, exist_ok=True)
    scatter_dir = out_dir / "scatter_plots"
    cm_dir      = out_dir / "conf_matrices"
    topomap_dir = out_dir / "topomaps"
    for d in (scatter_dir, cm_dir, topomap_dir):
        d.mkdir(parents=True, exist_ok=True)

    # 1) LOAD & SEGMENT
    data, labels = load_deap_dat(dat_path)
    X_seg, y_seg = segment_trials(data, labels)

    # 2) FEATURE EXTRACTION
    feats_df = extract_features(X_seg)
    feats_df.to_csv(out_dir / "features.csv", index=False)

    # 3) TOPOMAPLAR (out_dir yerine topomap_dir kullanÄ±lÄ±yor)
    feature_topomaps(
        feats_df,
        y_seg,
        topomap_dir,
        prefix=dat_path.stem,
        cnn_val_dir=cnn_val_dir,
        cnn_aro_dir=cnn_aro_dir,
        thresh=C.THRESH,
    )

    # 4) MODEL TRAIN & PREDICT
    results = _train_all(feats_df.values, y_seg, cv)
    # ---- AlexNet (2-D CNN) --------------------------------------------
    # Segment etiketlerini .npyâ€™ye kaydet â€“ APÄ° basitÃ§e (y_true, y_pred) bekliyor
    y_file = out_dir / "y_seg.npy"
    np.save(y_file, y_seg)

    # cnn_val_dir: feature_topomaps zaten 227Ã—227 PNGâ€™leri buraya kopyaladÄ±
    ax_model, ax_res = AX.train(
        cnn_val_dir,      # kÃ¶k klasÃ¶r (high/low alt klasÃ¶rleri iÃ§eriyor)
        y_file,
        verbose=0
    )
    results["alexnet"] = (ax_model, ax_res)

    metrics, cms = {}, {}
    classification_rows: List[Dict] = []

    for name, (mdl, (Xv, yv, yp)) in results.items():
        # --- Regresyon metrikleri
        reg = regression(yv, yp)
        metrics.update({f"{k}_{name}": v for k, v in reg.items()})

        # --- DaÄŸÄ±lÄ±m (scatter) grafikleri â†’ scatter_plots/
        scatter_plot(
            yv[:, 0], yp[:, 0],
            f"{name.upper()} Val",
            scatter_dir / f"sc_val_{name}.png"
        )
        scatter_plot(
            yv[:, 1], yp[:, 1],
            f"{name.upper()} Aro",
            scatter_dir / f"sc_aro_{name}.png"
        )

        # --- KarÄ±ÅŸÄ±klÄ±k matrisleri â†’ conf_matrices/
        yv_bin = binarize(yv, C.THRESH)
        yp_bin = binarize(yp, C.THRESH)
        cm_v, cm_a = conf_mats(yv_bin, yp_bin)
        confusion_matrix_heatmap(
            cm_v, f"{name} VAL",
            cm_dir / f"cm_val_{name}.png"
        )
        confusion_matrix_heatmap(
            cm_a, f"{name} ARO",
            cm_dir / f"cm_aro_{name}.png"
        )
        cms[name] = (cm_v, cm_a)

        # --- SÄ±nÄ±flandÄ±rma metrikleri (isteÄŸe baÄŸlÄ±)
        classification_rows.append({
            "model":   name,
            "acc_val": accuracy_score(yv_bin[:, 0], yp_bin[:, 0]),
            "prec_val":precision_score(yv_bin[:, 0], yp_bin[:, 0]),
            "rec_val":  recall_score(yv_bin[:, 0], yp_bin[:, 0]),
            "f1_val":   f1_score(yv_bin[:, 0], yp_bin[:, 0]),
            "acc_aro":  accuracy_score(yv_bin[:, 1], yp_bin[:, 1]),
            "prec_aro": precision_score(yv_bin[:, 1], yp_bin[:, 1]),
            "rec_aro":  recall_score(yv_bin[:, 1], yp_bin[:, 1]),
            "f1_aro":   f1_score(yv_bin[:, 1], yp_bin[:, 1]),
        })

    # 5) Regresyon metriklerini CSVâ€™ye yaz
    pd.DataFrame([metrics]).to_csv(
        out_dir / "regression_metrics.csv", index=False
    )

    # 6) SÄ±nÄ±flandÄ±rma metriklerini Excelâ€™e yaz (opsiyonel)
    df_cls = pd.DataFrame(classification_rows)
    excel_path = out_dir / "classification_metrics.xlsx"
    with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
        df_cls[
            ["model","acc_val","prec_val","rec_val","f1_val"]
        ].to_excel(writer, sheet_name="Valence", index=False)
        df_cls[
            ["model","acc_aro","prec_aro","rec_aro","f1_aro"]
        ].to_excel(writer, sheet_name="Arousal", index=False)

    return metrics, cms, {}

# --- Sabitler ---
_OUT_PX = 227
_DPI    = 100
_FIG_KW = dict(figsize=(_OUT_PX / _DPI, _OUT_PX / _DPI),
               frameon=False, dpi=_DPI)

# ------------------------------------------------------------------
def scatter_plot(y_true, y_pred, title: str, path: Path):
    plt.figure()
    plt.scatter(y_true, y_pred, alpha=0.6)
    plt.plot([1, 9], [1, 9], "--")
    plt.xlim(1, 9); plt.ylim(1, 9)
    plt.xlabel("Actual"); plt.ylabel("Predicted")
    plt.title(title); plt.tight_layout()
    plt.savefig(path, dpi=300); plt.close()

def confusion_matrix_heatmap(cm: np.ndarray, title: str, path: Path):
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d",
                cmap="Blues", cbar=False,
                xticklabels=["Low", "High"],
                yticklabels=["Low", "High"])
    plt.title(title); plt.tight_layout()
    plt.savefig(path, dpi=300); plt.close()

def plot_topomap(vals: np.ndarray, title: str, path: Path,
                 vmin=None, vmax=None, sensors=True):
    montage = mne.channels.make_standard_montage("standard_1020")
    info    = mne.create_info(DEAP_CH_NAMES, sfreq=FS,
                              ch_types=["eeg"] * N_CHANNELS)
    info.set_montage(montage, match_case=False, on_missing="warn")
    fig, ax = plt.subplots(**_FIG_KW)  # 227Ã—227 px
    ax.set_position([0, 0, 1, 1])  # kenarlÄ±klarÄ± sÄ±fÄ±rla
    im, _   = mne.viz.plot_topomap(vals,
                                   info,
                                   axes=ax,
                                   show=False,
                                   sensors=sensors,
                                   contours=0)
    if vmin is not None and vmax is not None:
        im.set_clim(vmin, vmax)
    ax.set_title(title)
    fig.savefig(path, dpi=_DPI, bbox_inches=None, pad_inches=0)
    plt.close(fig)

# ------------------------------------------------------------------
def feature_topomaps(feats_df, y_seg, out_dir: Path,
                     prefix: str, cnn_val_dir: Path, cnn_aro_dir: Path,
                     thresh: float):
    out_dir.mkdir(parents=True, exist_ok=True)

    vmins, vmaxs = {}, {}
    for stat in STATS:
        arr = feats_df[[f"{stat}_{ch}" for ch in range(N_CHANNELS)]].values
        vmins[stat], vmaxs[stat] = np.nanmin(arr), np.nanmax(arr)

    # Segment-bazlÄ± topomap + CNN kopyalama
    if y_seg.size:
        for seg_idx, row in feats_df.iterrows():
            val_lbl = "high" if y_seg[seg_idx, 0] > thresh else "low"
            aro_lbl = "high" if y_seg[seg_idx, 1] > thresh else "low"
            for stat in STATS:
                vals = row[[f"{stat}_{ch}" for ch in range(N_CHANNELS)]].values
                vmin, vmax = vmins[stat], vmaxs[stat]
                fn_base = f"{prefix}_{stat}_seg{seg_idx:03d}.png"
                plot_topomap(
                    vals, "", out_dir / fn_base,
                    vmin=vmin, vmax=vmax, sensors=False
                )
                (cnn_val_dir / val_lbl).mkdir(parents=True, exist_ok=True)
                (cnn_aro_dir / aro_lbl).mkdir(parents=True, exist_ok=True)
                src = out_dir / fn_base
                val_dst = cnn_val_dir / val_lbl / fn_base
                aro_dst = cnn_aro_dir / aro_lbl / fn_base
                shutil.copy2(src, val_dst)  # kopyala (taÅŸÄ±ma yok)
                shutil.copy2(src, aro_dst)  # kopyala (ikinci de baÅŸarÄ±lÄ±)

    # Ortalama topomap
    mean_row = feats_df.mean()
    for stat in STATS:
        vals = mean_row[[f"{stat}_{ch}" for ch in range(N_CHANNELS)]].values
        plot_topomap(vals, f"{stat} (mean)", out_dir / f"{stat}_mean.png",
                     vmin=vmins[stat], vmax=vmaxs[stat], sensors=False)

def _build(input_shape=(227,227,3)) -> tf.keras.Model:
    m = models.Sequential([
        layers.Input(shape=input_shape),
        # â€¦ your AlexNet conv blocks here â€¦
        layers.Conv2D(96, 11, strides=4, activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPool2D(3,2),
        layers.Conv2D(256, 5, padding="same", activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPool2D(3,2),
        layers.Conv2D(384, 3, padding="same", activation="relu"),
        layers.Conv2D(384, 3, padding="same", activation="relu"),
        layers.Conv2D(256, 3, padding="same", activation="relu"),
        layers.MaxPool2D(3,2),
        layers.Flatten(),
        layers.Dense(1024, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(512, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(2, activation="linear"),
    ])
    m.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
              loss="mse", metrics=["mae"])
    return m

def train(
    topomap_root: Path,
    y_file: Path,
    *,
    batch_size: int = 32,
    epochs: int = 30,
    verbose: int = 0
) -> Tuple[tf.keras.Model, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
    # 1) Load & normalize labels
    y_raw = np.load(y_file)                     # shape (N,2)
    y_min, y_max = y_raw.min(axis=0), y_raw.max(axis=0)
    y_norm = (y_raw - y_min) / (y_max - y_min)
    N = len(y_norm)

    # 2) Gather all filepaths in a stable order
    aug = ImageDataGenerator(rescale=1/255.0,
                             rotation_range=20,
                             width_shift_range=0.15,
                             height_shift_range=0.15,
                             zoom_range=0.15,
                             horizontal_flip=True,
                             fill_mode="nearest")
    flow = aug.flow_from_directory(
        topomap_root,
        target_size=(227,227),
        batch_size=1,
        class_mode=None,
        shuffle=False
    )
    filepaths = np.array(flow.filepaths)

    # 3) Split indices 80/20
    idx = np.arange(N)
    np.random.shuffle(idx)
    split = int(0.8 * N)
    tr_idx, vl_idx = idx[:split], idx[split:]

    # 4) A Python generator that loads only batch_size images at a time
    def gen(indices):
        while True:
            np.random.shuffle(indices)
            for start in range(0, len(indices), batch_size):
                batch = indices[start:start+batch_size]
                Xb = np.stack([
                    img_to_array(load_img(str(filepaths[i]), (227,227))) / 255.0
                    for i in batch
                ])
                yb = y_norm[batch]
                yield Xb, yb

    train_gen = gen(tr_idx)
    val_gen   = gen(vl_idx)
    steps_per_epoch = len(tr_idx) // batch_size
    val_steps       = max(1, len(vl_idx) // batch_size)

    # 5) Build & fit
    model = _build()
    es = callbacks.EarlyStopping(patience=5, restore_best_weights=True)
    model.fit(
        train_gen,
        steps_per_epoch=steps_per_epoch,
        validation_data=val_gen,
        validation_steps=val_steps,
        epochs=epochs,
        callbacks=[es],
        verbose=verbose
    )

    # 6) Predict on validation generator
    preds_norm = model.predict(val_gen, steps=val_steps, verbose=0)
    preds_raw  = preds_norm * (y_max - y_min) + y_min
    y_val_raw  = y_raw[vl_idx[: val_steps * batch_size]]

    # Return y_val & preds for metrics (we donâ€™t need to return X_val)
    return model, (None, y_val_raw, preds_raw)

"""
classical.py
------------
DEAP regresyonu iÃ§in klasik (tabular) model pipelineâ€™larÄ±
+ 5-Fold CVâ€™de kullanÄ±lmak Ã¼zere parametre Ä±zgaralarÄ±.

Fonksiyonlar
------------
rf()   -> Random Forest pipeline
xgb()  -> XGBoost  pipeline
svr()  -> SVR       pipeline
knn()  -> k-NN      pipeline
get_models_and_grids() -> [(name, pipeline, param_grid), ...]
"""
# --------------------------------------------------------------------------- #
# HazÄ±r pipeline tanÄ±mlarÄ±
# --------------------------------------------------------------------------- #
def rf():
    return Pipeline([
        ("scale", StandardScaler()),
        ("rf", MultiOutputRegressor(
            RandomForestRegressor(
                n_estimators=300,
                random_state=42,
                n_jobs=-1)))
    ])


def xgb():
    return Pipeline([
        ("scale", StandardScaler()),
        ("xgb", MultiOutputRegressor(
            XGBRegressor(
                n_estimators=300,
                random_state=42,
                n_jobs=-1,
                objective="reg:squarederror",
                tree_method="hist")))
    ])


def svr():
    return Pipeline([
        ("scale", StandardScaler()),
        ("svr", MultiOutputRegressor(
            SVR(kernel="rbf")))
    ])


def knn():
    return Pipeline([
        ("scale", StandardScaler()),
        ("knn", MultiOutputRegressor(
            KNeighborsRegressor(
                n_neighbors=5,
                n_jobs=-1)))
    ])


# --------------------------------------------------------------------------- #
# 5-Fold CVâ€™de kullanÄ±lacak liste
# --------------------------------------------------------------------------- #
def get_models_and_grids():
    """
    Her model iÃ§in (name, pipeline, param_grid) Ã¼Ã§lÃ¼sÃ¼ dÃ¶ndÃ¼rÃ¼r.
    """
    models = []

    # Random Forest
    rf_pipe = rf()
    rf_grid = {
        "rf__estimator__n_estimators": [100, 300],
        "rf__estimator__max_depth":   [None, 10, 20],
    }
    models.append(("RandomForest", rf_pipe, rf_grid))

    # SVR
    svr_pipe = svr()
    svr_grid = {
        "svr__estimator__C":     [1.0, 10.0],
        "svr__estimator__gamma": ["scale", 0.01],
    }
    models.append(("SVR", svr_pipe, svr_grid))

    # k-NN
    knn_pipe = knn()
    knn_grid = {
        "knn__estimator__n_neighbors": [3, 7, 15],
        "knn__estimator__weights":     ["uniform", "distance"],
    }
    models.append(("kNN", knn_pipe, knn_grid))

    # XGBoost
    xgb_pipe = xgb()
    xgb_grid = {
        "xgb__estimator__n_estimators":  [100, 300],
        "xgb__estimator__max_depth":     [3, 6],
        "xgb__estimator__learning_rate": [0.1, 0.01],
    }
    models.append(("XGB", xgb_pipe, xgb_grid))

    return models

def build(input_dim: int) -> tf.keras.Model:
    m = models.Sequential([
        layers.Input((input_dim,)),
        layers.Dense(256, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(64, activation="relu"),
        layers.Dense(2, activation="linear"),
    ])
    m.compile(optimizer="adam", loss="mse", metrics=["mae"])
    return m

def train(X: np.ndarray, y: np.ndarray,
          aux_preds: np.ndarray, *, verbose: int = 0
          ) -> Tuple[tf.keras.Model, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
    X_aug = np.concatenate([X, aux_preds], axis=1)
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_aug, y, test_size=0.2, random_state=42
    )
    m = build(X_aug.shape[1])
    es = callbacks.EarlyStopping(patience=15, restore_best_weights=True)
    m.fit(X_tr, y_tr, validation_data=(X_val, y_val),
          epochs=200, batch_size=32, callbacks=[es], verbose=verbose)
    preds = m.predict(X_val, verbose=0)
    return m, (X_val, y_val, preds)

import numpy as np
import pandas as pd
import pickle
import csv
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# ðŸ“‚ Ã‡Ä±ktÄ± klasÃ¶rÃ¼ oluÅŸtur
os.makedirs("outputs", exist_ok=True)

# ðŸŽ¨ Confusion matrix Ã§izim fonksiyonu
def save_confusion_matrix(y_true, y_pred, title, filename):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Low", "High"])
    fig, ax = plt.subplots(figsize=(5, 4))
    disp.plot(ax=ax, cmap='Blues', values_format='d')
    plt.title(title)
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.close()

# ðŸ”µ Scatter plot (gerÃ§ek arousal/valence deÄŸeri vs tahmin edilen High olasÄ±lÄ±ÄŸÄ±)
def save_scatter_plot_regression_style(y_true_cont, y_pred_prob, title, filename):
    plt.figure()
    plt.scatter(y_true_cont, y_pred_prob, alpha=0.6)
    plt.plot([1, 9], [1, 9], "--", color="gray")
    plt.yticks(range(1, 10))
    plt.ylim(1, 9)
    plt.xticks(range(1, 10))
    plt.xlim(1, 9)
    plt.xlabel("Actual (1â€“9 scale)")
    plt.ylabel("Predicted Probability (High)")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.close()

# ðŸ§  Model eÄŸitimi ve Ã§Ä±ktÄ±larÄ± Ã¼retme fonksiyonu
def train_save_and_evaluate(X, y_cat, y_bin, y_cont, model_path, prefix):
    X_train, X_test, y_train, y_test, yb_train, yb_test, yc_train, yc_test = train_test_split(
        X, y_cat, y_bin, y_cont, test_size=0.2, random_state=42)

    model = Sequential([
        Conv2D(16, (2, 2), activation='relu', input_shape=(12, 10, 1)),
        MaxPooling2D((2, 2)),
        Conv2D(32, (2, 2), activation='relu'),
        Flatten(),
        Dense(64, activation='relu'),
        Dropout(0.4),
        Dense(2, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=0)
    model.save(model_path)

    y_pred_prob = model.predict(X_test)
    y_pred_class = np.argmax(y_pred_prob, axis=1)
    y_true_class = np.argmax(y_test, axis=1)

    save_confusion_matrix(y_true_class, y_pred_class, f"Confusion Matrix - {prefix}", f"outputs/{prefix}_cm.png")
    # 0â€“1 arasÄ±ndaki tahminleri 1â€“9 aralÄ±ÄŸÄ±na Ã¶lÃ§ekle
    y_pred_rescaled = y_pred_prob[:, 1] * 8 + 1
    save_scatter_plot_regression_style(yc_test, y_pred_rescaled, f"Scatter Plot - {prefix}",
                                       f"outputs/{prefix}_scatter.png")

    acc = np.mean(y_pred_class == y_true_class)
    return round(acc, 4)

# ðŸ§ª Ã–zellik tÃ¼rleri ve kanal indeksleri
feature_types = ['activity', 'mobility', 'complexity',
                 'skew', 'kurt', 'delta', 'theta', 'alpha', 'beta', 'gamma']
selected_channels = [2, 0, 16, 17, 3, 19, 11, 27, 10, 13, 29, 26]

results = []

# ðŸ” TÃ¼m katÄ±lÄ±mcÄ±lar iÃ§in iÅŸle
for subject_id in range(1, 33):
    print(f"\nðŸ“¦ KatÄ±lÄ±mcÄ± s{subject_id:02d} iÅŸleniyor...")

    dat_path = f"data/s{subject_id:02d}.dat"
    feature_path = f"data/features_s{subject_id:02d}.csv"

    with open(dat_path, 'rb') as f:
        data = pickle.load(f, encoding='latin1')

    df = pd.read_csv(feature_path)
    scaler = MinMaxScaler()
    df[df.columns] = scaler.fit_transform(df[df.columns])

    labels = data['labels']
    arousal = labels[:, 0]  # continuous arousal (1â€“9)
    valence = labels[:, 1]  # continuous valence (1â€“9)

    y_arousal_bin = (arousal >= 5).astype(int)
    y_valence_bin = (valence >= 5).astype(int)

    y_arousal_full = np.repeat(y_arousal_bin, 15)
    y_valence_full = np.repeat(y_valence_bin, 15)
    y_arousal_cat = to_categorical(y_arousal_full, num_classes=2)
    y_valence_cat = to_categorical(y_valence_full, num_classes=2)
    y_arousal_cont = np.repeat(arousal, 15)
    y_valence_cont = np.repeat(valence, 15)

    X = np.zeros((600, 12, 10, 1))
    for i, feature in enumerate(feature_types):
        for ch_i, ch in enumerate(selected_channels):
            col_name = f"{feature}_{ch}"
            X[:, ch_i, i, 0] = df[col_name].values

    acc_arousal = train_save_and_evaluate(
        X, y_arousal_cat, y_arousal_full, y_arousal_cont,
        f"outputs/cnn_arousal_s{subject_id:02d}.h5",
        f"s{subject_id:02d}_arousal")

    acc_valence = train_save_and_evaluate(
        X, y_valence_cat, y_valence_full, y_valence_cont,
        f"outputs/cnn_valence_s{subject_id:02d}.h5",
        f"s{subject_id:02d}_valence")

    results.append([f"s{subject_id:02d}", acc_arousal, acc_valence])

# ðŸ“„ BaÅŸarÄ±larÄ± CSV'ye yaz
with open("outputs/model_accuracies.csv", "w", newline="") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["Participant", "Arousal Accuracy", "Valence Accuracy"])
    writer.writerows(results)

print("\nâœ… TÃ¼m modeller eÄŸitildi, .h5 dosyalarÄ± ve grafik Ã§Ä±ktÄ±larÄ± 'outputs/' klasÃ¶rÃ¼ne kaydedildi.")