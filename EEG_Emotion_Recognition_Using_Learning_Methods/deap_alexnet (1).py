# -*- coding: utf-8 -*-
"""DEAP - Alexnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hMiTM3k_f7bxE7CdWsLYEHkXdwLVzZrO
"""

!pip install mne

# ── Standart Kütüphaneler ────────────────────────────
import argparse
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import shutil
from typing import Any, Dict, List, Tuple

# ── Bilimsel / Sayısal Ekosistem ─────────────────────
import numpy as np
import pandas as pd
from scipy.signal import welch
from scipy.stats import skew, kurtosis

import matplotlib
matplotlib.use("Agg")          # GUI'siz arka uç
import matplotlib.pyplot as plt
import seaborn as sns
import mne

# ── Makine / Derin Öğrenme ──────────────────────────
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.preprocessing.image import (
    ImageDataGenerator, load_img, img_to_array
)

from sklearn.model_selection import (
    train_test_split, KFold, cross_validate
)
from sklearn.metrics import (
    mean_absolute_error, r2_score,
    accuracy_score, precision_score,
    recall_score, f1_score, confusion_matrix
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR

import xgboost as xgb
from xgboost import XGBRegressor

# ── Excel Çıkışı İçin ───────────────────────────────
import openpyxl

matplotlib.use("Agg")          # GUI'siz, headless backend

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", required=True)
    ap.add_argument("--out", default="outputs_batch")
    ap.add_argument("--subject", type=int, help="Yalnızca belirtilen deneği işler (1-32)")
    ap.add_argument("--cv", action="store_true", help="%%20/%%80 hold-out yerine 5-Fold CV ile değerlendir.")
    args = ap.parse_args()

    data_dir = Path(args.data_dir).expanduser().resolve()
    out_root = Path(args.out).expanduser().resolve()
    out_root.mkdir(parents=True, exist_ok=True)

    cnn_val_dir = out_root / C.CNN_VAL_DIR
    cnn_aro_dir = out_root / C.CNN_ARO_DIR

    # Hangi denekler işlenecek?
    if args.subject:  # tek denek modu
        idxs = [args.subject]
    else:  # tam kohort
        idxs = range(1, 33)

    tasks = [
        (i,
         data_dir / f"s{i:02d}.dat",
         out_root / f"subj_{i:02d}",
         cnn_val_dir,
         cnn_aro_dir,
         args.cv)
        for i in idxs
    ]

    all_metrics, cm_tot = [], {m: {'val': np.zeros((2,2), int),
                                   'aro': np.zeros((2,2), int)}
                               for m in C.MODELS}

    with ProcessPoolExecutor() as ex:
        futs = {ex.submit(process_subject, *t): t[0] for t in tasks if t[1].exists()}
        for fut in as_completed(futs):
            idx = futs[fut]
            print(f"✓ subj_{idx:02d}")
            mets, cms, _ = fut.result()
            all_metrics.append(mets)
            for m in C.MODELS:
                cm_tot[m]['val'] += cms[m][0]
                cm_tot[m]['aro'] += cms[m][1]

    # ----- toplu sonuçlar -----
    df = pd.DataFrame(all_metrics)
    df.loc['mean'] = df.mean()
    df.to_csv(out_root / "metrics.csv", index=False)
    print(df.loc['mean'].round(3))

if __name__ == "__main__":
    import tensorflow as tf
    tf.get_logger().setLevel("ERROR")
    main()

# DSP & sinyal
FS        = 128
WIN_SEC   = 4
N_CHANNELS = 12
THRESH     = 4.5

BANDS = {
    "delta": (1, 4),
    "theta": (4, 8),
    "alpha": (8, 12),
    "beta":  (12, 30),
    "gamma": (30, 45),
}

DEAP_CH_NAMES = [
    "F7", "Fp1", "Fp2", "F8",
    "F3",  "F4",
    "P3",  "P4",
    "P7", "O1", "O2", "P8",
]

MODELS      = ['rf','xgb','svr','knn','dnn','alexnet']
MODELS_IMP  = ['rf','xgb','svr','knn']
STATS       = ["activity","mobility","complexity","skew","kurt"] + list(BANDS.keys())

# Çıktı dizinlerinin isimleri
CNN_VAL_DIR = "cnn_valence"
CNN_ARO_DIR = "cnn_arousal"

ROOT = Path(__file__).resolve().parent

def load_deap_dat(path: Path) -> Tuple[np.ndarray, np.ndarray]:
    with path.open("rb") as f:
        d = pickle.load(f, encoding="latin1")
    data   = d["data"].astype(np.float32)[:, :N_CHANNELS, :]
    labels = d["labels"][:, :2].astype(np.float32)
    if data.shape[-1] == 8064:
        data = data[:, :, 3*FS:]          # ilk 3 s-i at
    assert data.shape == (40, N_CHANNELS, 60*FS)
    return data, labels

def segment_trials(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    samp_win = WIN_SEC * FS
    X, y = [], []
    for t in range(data.shape[0]):
        for w in range(60 // WIN_SEC):
            s, e = w * samp_win, (w + 1) * samp_win
            X.append(data[t, :, s:e])
            y.append(labels[t])
    return np.stack(X), np.stack(y)

def _bandpower(epoch: np.ndarray, band: Tuple[int, int]) -> float:
    freqs, psd = welch(epoch, fs=FS, nperseg=FS*2)
    fmin, fmax = band
    return psd[(freqs >= fmin) & (freqs <= fmax)].mean()

def extract_features(X_seg: np.ndarray) -> pd.DataFrame:
    n_win, n_ch, _ = X_seg.shape
    feats = []
    for w in range(n_win):
        ch_feats = []
        for ch in range(n_ch):
            sig = X_seg[w, ch]
            var_sig = np.var(sig)
            diff1   = np.diff(sig); var_d1 = np.var(diff1)
            mobility = np.sqrt(var_d1 / var_sig) if var_sig else 0
            diff2    = np.diff(diff1); var_d2 = np.var(diff2)
            mobility2 = np.sqrt(var_d2 / var_d1) if var_d1 else 0
            complexity = mobility2 / mobility if mobility else 0
            ch_feats += [
                var_sig, mobility, complexity,
                skew(sig), kurtosis(sig)
            ]
            ch_feats += [_bandpower(sig, b) for b in BANDS.values()]
        feats.append(ch_feats)

    cols = [f"{stat}_{ch}" for ch in range(n_ch) for stat in STATS]
    return pd.DataFrame(feats, columns=cols)

def regression(y_true, y_pred):
    return {
        'mae_val': mean_absolute_error(y_true[:,0], y_pred[:,0]),
        'mae_aro': mean_absolute_error(y_true[:,1], y_pred[:,1]),
        'r2_val' : r2_score(y_true[:,0],  y_pred[:,0]),
        'r2_aro' : r2_score(y_true[:,1],  y_pred[:,1]),
    }

def conf_mats(y_true, y_pred_bin):
    cm_v = confusion_matrix(y_true[:,0], y_pred_bin[:,0])
    cm_a = confusion_matrix(y_true[:,1], y_pred_bin[:,1])
    return cm_v, cm_a

def binarize(arr, thresh):
    return (arr > thresh).astype(int)

def evaluate_cv(
    pipeline,
    X: np.ndarray,
    y: np.ndarray,
    cv_splits: int = 5
) -> tuple[float, float]:
    """
    Runs KFold CV on (X, y) with the given pipeline.
    Returns (mean_test_mae, mean_test_r2).
    """
    cv = KFold(n_splits=cv_splits, shuffle=True, random_state=42)
    scoring = {
        'mae': make_scorer(mean_absolute_error),
        'r2' : make_scorer(r2_score),
    }
    cv_res = cross_validate(
        pipeline, X, y,
        cv=cv,
        scoring=scoring,
        return_train_score=False,
        n_jobs=-1
    )
    return cv_res['test_mae'].mean(), cv_res['test_r2'].mean()

def _train_all(
    feats: np.ndarray,
    y: np.ndarray,
    cv: bool = False
) -> Dict[str, Tuple[object, Tuple[np.ndarray, np.ndarray, np.ndarray]]]:
    """
    RF, XGB, SVR, k-NN ve DNN’i eğitir; doğrulama kümesindeki
    tahminleri döner.  AlexNet buraya **eklenmiyor** – aşağıda,
    process_subject içinde çağrılacak.
    """
    X_tr, X_val, y_tr, y_val = train_test_split(
        feats, y, test_size=0.2, random_state=42
    )

    results: Dict[str, Tuple[object, Tuple[np.ndarray, np.ndarray, np.ndarray]]] = {}

    # -- Klasik modeller -------------------------------------------------
    for name in C.MODELS_IMP:              # ['rf','xgb','svr','knn']
        pipe = getattr(M, name)()
        pipe.fit(X_tr, y_tr)
        preds = pipe.predict(X_val)
        results[name] = (pipe, (X_val, y_val, preds))

    # -- DNN --------------------------------------------------------------
    rf_preds = results["rf"][0].predict(feats)    # RF çıktılarını ek özellik yap
    dnn_model, dnn_res = D.train(feats, y, rf_preds)
    results["dnn"] = (dnn_model, dnn_res)

    return results



def process_subject(
    idx: int,
    dat_path: Path,
    out_dir: Path,
    cnn_val_dir: Path,
    cnn_aro_dir: Path,
    cv: bool = False,
) -> Tuple[Dict, Dict, Dict]:
    """
    Tek bir denek için:
      1) Veri yükle & segmentle
      2) Özellik çıkar, kaydet
      3) Topomap’ları oluştur ve cnn klasörlerine kopyala
      4) Modelleri eğit & tahmin et
      5) Regresyon & sınıflandırma metriklerini hesapla
      6) Tüm grafik çıktıları alt klasörlere kaydet
    """
    # 0) ALT KLASÖRLERİ HAZIRLA
    out_dir.mkdir(parents=True, exist_ok=True)
    scatter_dir = out_dir / "scatter_plots"
    cm_dir      = out_dir / "conf_matrices"
    topomap_dir = out_dir / "topomaps"
    for d in (scatter_dir, cm_dir, topomap_dir):
        d.mkdir(parents=True, exist_ok=True)

    # 1) LOAD & SEGMENT
    data, labels = load_deap_dat(dat_path)
    X_seg, y_seg = segment_trials(data, labels)

    # 2) FEATURE EXTRACTION
    feats_df = extract_features(X_seg)
    feats_df.to_csv(out_dir / "features.csv", index=False)

    # 3) TOPOMAPLAR (out_dir yerine topomap_dir kullanılıyor)
    feature_topomaps(
        feats_df,
        y_seg,
        topomap_dir,
        prefix=dat_path.stem,
        cnn_val_dir=cnn_val_dir,
        cnn_aro_dir=cnn_aro_dir,
        thresh=C.THRESH,
    )

    # 4) MODEL TRAIN & PREDICT
    results = _train_all(feats_df.values, y_seg, cv)
    # ---- AlexNet (2-D CNN) --------------------------------------------
    # Segment etiketlerini .npy’ye kaydet – APİ basitçe (y_true, y_pred) bekliyor
    y_file = out_dir / "y_seg.npy"
    np.save(y_file, y_seg)

    # cnn_val_dir: feature_topomaps zaten 227×227 PNG’leri buraya kopyaladı
    ax_model, ax_res = AX.train(
        cnn_val_dir,      # kök klasör (high/low alt klasörleri içeriyor)
        y_file,
        verbose=0
    )
    results["alexnet"] = (ax_model, ax_res)

    metrics, cms = {}, {}
    classification_rows: List[Dict] = []

    for name, (mdl, (Xv, yv, yp)) in results.items():
        # --- Regresyon metrikleri
        reg = regression(yv, yp)
        metrics.update({f"{k}_{name}": v for k, v in reg.items()})

        # --- Dağılım (scatter) grafikleri → scatter_plots/
        scatter_plot(
            yv[:, 0], yp[:, 0],
            f"{name.upper()} Val",
            scatter_dir / f"sc_val_{name}.png"
        )
        scatter_plot(
            yv[:, 1], yp[:, 1],
            f"{name.upper()} Aro",
            scatter_dir / f"sc_aro_{name}.png"
        )

        # --- Karışıklık matrisleri → conf_matrices/
        yv_bin = binarize(yv, C.THRESH)
        yp_bin = binarize(yp, C.THRESH)
        cm_v, cm_a = conf_mats(yv_bin, yp_bin)
        confusion_matrix_heatmap(
            cm_v, f"{name} VAL",
            cm_dir / f"cm_val_{name}.png"
        )
        confusion_matrix_heatmap(
            cm_a, f"{name} ARO",
            cm_dir / f"cm_aro_{name}.png"
        )
        cms[name] = (cm_v, cm_a)

        # --- Sınıflandırma metrikleri (isteğe bağlı)
        classification_rows.append({
            "model":   name,
            "acc_val": accuracy_score(yv_bin[:, 0], yp_bin[:, 0]),
            "prec_val":precision_score(yv_bin[:, 0], yp_bin[:, 0]),
            "rec_val":  recall_score(yv_bin[:, 0], yp_bin[:, 0]),
            "f1_val":   f1_score(yv_bin[:, 0], yp_bin[:, 0]),
            "acc_aro":  accuracy_score(yv_bin[:, 1], yp_bin[:, 1]),
            "prec_aro": precision_score(yv_bin[:, 1], yp_bin[:, 1]),
            "rec_aro":  recall_score(yv_bin[:, 1], yp_bin[:, 1]),
            "f1_aro":   f1_score(yv_bin[:, 1], yp_bin[:, 1]),
        })

    # 5) Regresyon metriklerini CSV’ye yaz
    pd.DataFrame([metrics]).to_csv(
        out_dir / "regression_metrics.csv", index=False
    )

    # 6) Sınıflandırma metriklerini Excel’e yaz (opsiyonel)
    df_cls = pd.DataFrame(classification_rows)
    excel_path = out_dir / "classification_metrics.xlsx"
    with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
        df_cls[
            ["model","acc_val","prec_val","rec_val","f1_val"]
        ].to_excel(writer, sheet_name="Valence", index=False)
        df_cls[
            ["model","acc_aro","prec_aro","rec_aro","f1_aro"]
        ].to_excel(writer, sheet_name="Arousal", index=False)

    return metrics, cms, {}

# --- Sabitler ---
_OUT_PX = 227
_DPI    = 100
_FIG_KW = dict(figsize=(_OUT_PX / _DPI, _OUT_PX / _DPI),
               frameon=False, dpi=_DPI)

# ------------------------------------------------------------------
def scatter_plot(y_true, y_pred, title: str, path: Path):
    plt.figure()
    plt.scatter(y_true, y_pred, alpha=0.6)
    plt.plot([1, 9], [1, 9], "--")
    plt.xlim(1, 9); plt.ylim(1, 9)
    plt.xlabel("Actual"); plt.ylabel("Predicted")
    plt.title(title); plt.tight_layout()
    plt.savefig(path, dpi=300); plt.close()

def confusion_matrix_heatmap(cm: np.ndarray, title: str, path: Path):
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d",
                cmap="Blues", cbar=False,
                xticklabels=["Low", "High"],
                yticklabels=["Low", "High"])
    plt.title(title); plt.tight_layout()
    plt.savefig(path, dpi=300); plt.close()

def plot_topomap(vals: np.ndarray, title: str, path: Path,
                 vmin=None, vmax=None, sensors=True):
    montage = mne.channels.make_standard_montage("standard_1020")
    info    = mne.create_info(DEAP_CH_NAMES, sfreq=FS,
                              ch_types=["eeg"] * N_CHANNELS)
    info.set_montage(montage, match_case=False, on_missing="warn")
    fig, ax = plt.subplots(**_FIG_KW)  # 227×227 px
    ax.set_position([0, 0, 1, 1])  # kenarlıkları sıfırla
    im, _   = mne.viz.plot_topomap(vals,
                                   info,
                                   axes=ax,
                                   show=False,
                                   sensors=sensors,
                                   contours=0)
    if vmin is not None and vmax is not None:
        im.set_clim(vmin, vmax)
    ax.set_title(title)
    fig.savefig(path, dpi=_DPI, bbox_inches=None, pad_inches=0)
    plt.close(fig)

# ------------------------------------------------------------------
def feature_topomaps(feats_df, y_seg, out_dir: Path,
                     prefix: str, cnn_val_dir: Path, cnn_aro_dir: Path,
                     thresh: float):
    out_dir.mkdir(parents=True, exist_ok=True)

    vmins, vmaxs = {}, {}
    for stat in STATS:
        arr = feats_df[[f"{stat}_{ch}" for ch in range(N_CHANNELS)]].values
        vmins[stat], vmaxs[stat] = np.nanmin(arr), np.nanmax(arr)

    # Segment-bazlı topomap + CNN kopyalama
    if y_seg.size:
        for seg_idx, row in feats_df.iterrows():
            val_lbl = "high" if y_seg[seg_idx, 0] > thresh else "low"
            aro_lbl = "high" if y_seg[seg_idx, 1] > thresh else "low"
            for stat in STATS:
                vals = row[[f"{stat}_{ch}" for ch in range(N_CHANNELS)]].values
                vmin, vmax = vmins[stat], vmaxs[stat]
                fn_base = f"{prefix}_{stat}_seg{seg_idx:03d}.png"
                plot_topomap(
                    vals, "", out_dir / fn_base,
                    vmin=vmin, vmax=vmax, sensors=False
                )
                (cnn_val_dir / val_lbl).mkdir(parents=True, exist_ok=True)
                (cnn_aro_dir / aro_lbl).mkdir(parents=True, exist_ok=True)
                src = out_dir / fn_base
                val_dst = cnn_val_dir / val_lbl / fn_base
                aro_dst = cnn_aro_dir / aro_lbl / fn_base
                shutil.copy2(src, val_dst)  # kopyala (taşıma yok)
                shutil.copy2(src, aro_dst)  # kopyala (ikinci de başarılı)

    # Ortalama topomap
    mean_row = feats_df.mean()
    for stat in STATS:
        vals = mean_row[[f"{stat}_{ch}" for ch in range(N_CHANNELS)]].values
        plot_topomap(vals, f"{stat} (mean)", out_dir / f"{stat}_mean.png",
                     vmin=vmins[stat], vmax=vmaxs[stat], sensors=False)

def _build(input_shape=(227,227,3)) -> tf.keras.Model:
    m = models.Sequential([
        layers.Input(shape=input_shape),
        # … your AlexNet conv blocks here …
        layers.Conv2D(96, 11, strides=4, activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPool2D(3,2),
        layers.Conv2D(256, 5, padding="same", activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPool2D(3,2),
        layers.Conv2D(384, 3, padding="same", activation="relu"),
        layers.Conv2D(384, 3, padding="same", activation="relu"),
        layers.Conv2D(256, 3, padding="same", activation="relu"),
        layers.MaxPool2D(3,2),
        layers.Flatten(),
        layers.Dense(1024, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(512, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(2, activation="linear"),
    ])
    m.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
              loss="mse", metrics=["mae"])
    return m

def train(
    topomap_root: Path,
    y_file: Path,
    *,
    batch_size: int = 32,
    epochs: int = 30,
    verbose: int = 0
) -> Tuple[tf.keras.Model, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
    # 1) Load & normalize labels
    y_raw = np.load(y_file)                     # shape (N,2)
    y_min, y_max = y_raw.min(axis=0), y_raw.max(axis=0)
    y_norm = (y_raw - y_min) / (y_max - y_min)
    N = len(y_norm)

    # 2) Gather all filepaths in a stable order
    aug = ImageDataGenerator(rescale=1/255.0,
                             rotation_range=20,
                             width_shift_range=0.15,
                             height_shift_range=0.15,
                             zoom_range=0.15,
                             horizontal_flip=True,
                             fill_mode="nearest")
    flow = aug.flow_from_directory(
        topomap_root,
        target_size=(227,227),
        batch_size=1,
        class_mode=None,
        shuffle=False
    )
    filepaths = np.array(flow.filepaths)

    # 3) Split indices 80/20
    idx = np.arange(N)
    np.random.shuffle(idx)
    split = int(0.8 * N)
    tr_idx, vl_idx = idx[:split], idx[split:]

    # 4) A Python generator that loads only batch_size images at a time
    def gen(indices):
        while True:
            np.random.shuffle(indices)
            for start in range(0, len(indices), batch_size):
                batch = indices[start:start+batch_size]
                Xb = np.stack([
                    img_to_array(load_img(str(filepaths[i]), (227,227))) / 255.0
                    for i in batch
                ])
                yb = y_norm[batch]
                yield Xb, yb

    train_gen = gen(tr_idx)
    val_gen   = gen(vl_idx)
    steps_per_epoch = len(tr_idx) // batch_size
    val_steps       = max(1, len(vl_idx) // batch_size)

    # 5) Build & fit
    model = _build()
    es = callbacks.EarlyStopping(patience=5, restore_best_weights=True)
    model.fit(
        train_gen,
        steps_per_epoch=steps_per_epoch,
        validation_data=val_gen,
        validation_steps=val_steps,
        epochs=epochs,
        callbacks=[es],
        verbose=verbose
    )

    # 6) Predict on validation generator
    preds_norm = model.predict(val_gen, steps=val_steps, verbose=0)
    preds_raw  = preds_norm * (y_max - y_min) + y_min
    y_val_raw  = y_raw[vl_idx[: val_steps * batch_size]]

    # Return y_val & preds for metrics (we don’t need to return X_val)
    return model, (None, y_val_raw, preds_raw)

"""
classical.py
------------
DEAP regresyonu için klasik (tabular) model pipeline’ları
+ 5-Fold CV’de kullanılmak üzere parametre ızgaraları.

Fonksiyonlar
------------
rf()   -> Random Forest pipeline
xgb()  -> XGBoost  pipeline
svr()  -> SVR       pipeline
knn()  -> k-NN      pipeline
get_models_and_grids() -> [(name, pipeline, param_grid), ...]
"""
# --------------------------------------------------------------------------- #
# Hazır pipeline tanımları
# --------------------------------------------------------------------------- #
def rf():
    return Pipeline([
        ("scale", StandardScaler()),
        ("rf", MultiOutputRegressor(
            RandomForestRegressor(
                n_estimators=300,
                random_state=42,
                n_jobs=-1)))
    ])


def xgb():
    return Pipeline([
        ("scale", StandardScaler()),
        ("xgb", MultiOutputRegressor(
            XGBRegressor(
                n_estimators=300,
                random_state=42,
                n_jobs=-1,
                objective="reg:squarederror",
                tree_method="hist")))
    ])


def svr():
    return Pipeline([
        ("scale", StandardScaler()),
        ("svr", MultiOutputRegressor(
            SVR(kernel="rbf")))
    ])


def knn():
    return Pipeline([
        ("scale", StandardScaler()),
        ("knn", MultiOutputRegressor(
            KNeighborsRegressor(
                n_neighbors=5,
                n_jobs=-1)))
    ])


# --------------------------------------------------------------------------- #
# 5-Fold CV’de kullanılacak liste
# --------------------------------------------------------------------------- #
def get_models_and_grids():
    """
    Her model için (name, pipeline, param_grid) üçlüsü döndürür.
    """
    models = []

    # Random Forest
    rf_pipe = rf()
    rf_grid = {
        "rf__estimator__n_estimators": [100, 300],
        "rf__estimator__max_depth":   [None, 10, 20],
    }
    models.append(("RandomForest", rf_pipe, rf_grid))

    # SVR
    svr_pipe = svr()
    svr_grid = {
        "svr__estimator__C":     [1.0, 10.0],
        "svr__estimator__gamma": ["scale", 0.01],
    }
    models.append(("SVR", svr_pipe, svr_grid))

    # k-NN
    knn_pipe = knn()
    knn_grid = {
        "knn__estimator__n_neighbors": [3, 7, 15],
        "knn__estimator__weights":     ["uniform", "distance"],
    }
    models.append(("kNN", knn_pipe, knn_grid))

    # XGBoost
    xgb_pipe = xgb()
    xgb_grid = {
        "xgb__estimator__n_estimators":  [100, 300],
        "xgb__estimator__max_depth":     [3, 6],
        "xgb__estimator__learning_rate": [0.1, 0.01],
    }
    models.append(("XGB", xgb_pipe, xgb_grid))

    return models

def build(input_dim: int) -> tf.keras.Model:
    m = models.Sequential([
        layers.Input((input_dim,)),
        layers.Dense(256, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(64, activation="relu"),
        layers.Dense(2, activation="linear"),
    ])
    m.compile(optimizer="adam", loss="mse", metrics=["mae"])
    return m

def train(X: np.ndarray, y: np.ndarray,
          aux_preds: np.ndarray, *, verbose: int = 0
          ) -> Tuple[tf.keras.Model, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
    X_aug = np.concatenate([X, aux_preds], axis=1)
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_aug, y, test_size=0.2, random_state=42
    )
    m = build(X_aug.shape[1])
    es = callbacks.EarlyStopping(patience=15, restore_best_weights=True)
    m.fit(X_tr, y_tr, validation_data=(X_val, y_val),
          epochs=200, batch_size=32, callbacks=[es], verbose=verbose)
    preds = m.predict(X_val, verbose=0)
    return m, (X_val, y_val, preds)

import numpy as np
import pandas as pd
import pickle
import csv
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# 📂 Çıktı klasörü oluştur
os.makedirs("outputs", exist_ok=True)

# 🎨 Confusion matrix çizim fonksiyonu
def save_confusion_matrix(y_true, y_pred, title, filename):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Low", "High"])
    fig, ax = plt.subplots(figsize=(5, 4))
    disp.plot(ax=ax, cmap='Blues', values_format='d')
    plt.title(title)
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.close()

# 🔵 Scatter plot (gerçek arousal/valence değeri vs tahmin edilen High olasılığı)
def save_scatter_plot_regression_style(y_true_cont, y_pred_prob, title, filename):
    plt.figure()
    plt.scatter(y_true_cont, y_pred_prob, alpha=0.6)
    plt.plot([1, 9], [1, 9], "--", color="gray")
    plt.yticks(range(1, 10))
    plt.ylim(1, 9)
    plt.xticks(range(1, 10))
    plt.xlim(1, 9)
    plt.xlabel("Actual (1–9 scale)")
    plt.ylabel("Predicted Probability (High)")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.close()

# 🧠 Model eğitimi ve çıktıları üretme fonksiyonu
def train_save_and_evaluate(X, y_cat, y_bin, y_cont, model_path, prefix):
    X_train, X_test, y_train, y_test, yb_train, yb_test, yc_train, yc_test = train_test_split(
        X, y_cat, y_bin, y_cont, test_size=0.2, random_state=42)

    model = Sequential([
        Conv2D(16, (2, 2), activation='relu', input_shape=(12, 10, 1)),
        MaxPooling2D((2, 2)),
        Conv2D(32, (2, 2), activation='relu'),
        Flatten(),
        Dense(64, activation='relu'),
        Dropout(0.4),
        Dense(2, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=0)
    model.save(model_path)

    y_pred_prob = model.predict(X_test)
    y_pred_class = np.argmax(y_pred_prob, axis=1)
    y_true_class = np.argmax(y_test, axis=1)

    save_confusion_matrix(y_true_class, y_pred_class, f"Confusion Matrix - {prefix}", f"outputs/{prefix}_cm.png")
    # 0–1 arasındaki tahminleri 1–9 aralığına ölçekle
    y_pred_rescaled = y_pred_prob[:, 1] * 8 + 1
    save_scatter_plot_regression_style(yc_test, y_pred_rescaled, f"Scatter Plot - {prefix}",
                                       f"outputs/{prefix}_scatter.png")

    acc = np.mean(y_pred_class == y_true_class)
    return round(acc, 4)

# 🧪 Özellik türleri ve kanal indeksleri
feature_types = ['activity', 'mobility', 'complexity',
                 'skew', 'kurt', 'delta', 'theta', 'alpha', 'beta', 'gamma']
selected_channels = [2, 0, 16, 17, 3, 19, 11, 27, 10, 13, 29, 26]

results = []

# 🔁 Tüm katılımcılar için işle
for subject_id in range(1, 33):
    print(f"\n📦 Katılımcı s{subject_id:02d} işleniyor...")

    dat_path = f"data/s{subject_id:02d}.dat"
    feature_path = f"data/features_s{subject_id:02d}.csv"

    with open(dat_path, 'rb') as f:
        data = pickle.load(f, encoding='latin1')

    df = pd.read_csv(feature_path)
    scaler = MinMaxScaler()
    df[df.columns] = scaler.fit_transform(df[df.columns])

    labels = data['labels']
    arousal = labels[:, 0]  # continuous arousal (1–9)
    valence = labels[:, 1]  # continuous valence (1–9)

    y_arousal_bin = (arousal >= 5).astype(int)
    y_valence_bin = (valence >= 5).astype(int)

    y_arousal_full = np.repeat(y_arousal_bin, 15)
    y_valence_full = np.repeat(y_valence_bin, 15)
    y_arousal_cat = to_categorical(y_arousal_full, num_classes=2)
    y_valence_cat = to_categorical(y_valence_full, num_classes=2)
    y_arousal_cont = np.repeat(arousal, 15)
    y_valence_cont = np.repeat(valence, 15)

    X = np.zeros((600, 12, 10, 1))
    for i, feature in enumerate(feature_types):
        for ch_i, ch in enumerate(selected_channels):
            col_name = f"{feature}_{ch}"
            X[:, ch_i, i, 0] = df[col_name].values

    acc_arousal = train_save_and_evaluate(
        X, y_arousal_cat, y_arousal_full, y_arousal_cont,
        f"outputs/cnn_arousal_s{subject_id:02d}.h5",
        f"s{subject_id:02d}_arousal")

    acc_valence = train_save_and_evaluate(
        X, y_valence_cat, y_valence_full, y_valence_cont,
        f"outputs/cnn_valence_s{subject_id:02d}.h5",
        f"s{subject_id:02d}_valence")

    results.append([f"s{subject_id:02d}", acc_arousal, acc_valence])

# 📄 Başarıları CSV'ye yaz
with open("outputs/model_accuracies.csv", "w", newline="") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["Participant", "Arousal Accuracy", "Valence Accuracy"])
    writer.writerows(results)

print("\n✅ Tüm modeller eğitildi, .h5 dosyaları ve grafik çıktıları 'outputs/' klasörüne kaydedildi.")